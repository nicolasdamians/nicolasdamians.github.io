---
layout: post
title: "M3T3 - ELK - Spanish Version"
date: 2024-05-15 10:25:00 
categories:
    - posts
tags:
    - Master
    - Home_work
    - M3
    - monitor
    - tools
    - es
---

### 

ÍNDICE DE CONTENIDOS

[**Enunciado de la tarea 3**](#enunciado-de-la-tarea)

[**Despliegue de Stack ELK 4**](#despliegue-de-stack-elk)

> [Proceso de instalación de ELK + Kibana
> 4](#proceso-de-instalación-de-elk-kibana)
>
> [Elastic 4](#elastic)
>
> [Kibana 6](#kibana)
>
> [Logstash 8](#logstash)
>
> [Input 10](#input)
>
> [Filtros 11](#filtros)
>
> [Output 14](#output)
>
> [Configuración del servicio logstash 15](#configuración-del-servicio-logstash)

[**Dashboards Kibana 15**](#dashboards-kibana)

# Enunciado de la tarea

ELK se puede utilizar de diferentes formas:\
- Instalando las aplicaciones descargadas desde el repositorio de
elastic.co\
- Utilizando docker-compose para levantar los servicios
([[https://www.docker.elastic.co/]{.underline}](https://www.docker.elastic.co/))\
- Utilizando una instancia de Elastic Cloud\
- Simplemente descargando las aplicaciones y ejecutándose sin necesidad
de instalarlas

Esta actividad tiene el objetivo de interiorizar el funcionamiento de la
pila ELK monitorizando logs del sistema.

La tarea consiste y tiene un valor de:

\- Despliegue de ELK (10%)\
- Descripción del formato de los logs elegidos, indicando con un ejemplo
la estructura de una línea modelo (20 %)\
- Configuración de Logstash o beats utilizados (30%)\
- A partir de los datos ingestados, mostrar un dashboard de Kibana que
contenga al menos cuatro visualizaciones. Se proponen algunas de las
siguientes visualizaciones (40%):

◦ Número total de eventos.\
◦ Ficheros más habituales\
◦ Categorías más recurrentes\
◦ Etc.

En esta actividad se deberán usar, al menos, el input plugin file{}, el
filter plugin grok{} y el output plugin de elasticsearch{}.

# Despliegue de Stack ELK

El enfoque inicial que planteé fue desplegar y usar el Elastic Cloud,
cosa que me pareció extremadamente sencillo. Finalmente opte por
desplegar la siguiente infra en mi lab on prem:

La idea es dejar una VM con Logstash y otra con ELK + Kibana. Habrán dos
dispositivos físicos enviando mediante Syslog datos a Logstash, este los
enviará a ELK y Kibana los indexara y printara. Nota: Para este
ejercicio solamente exportare logs desde el Fortigate.

## Proceso de instalación de ELK + Kibana

[[https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html]{.underline}](https://www.elastic.co/guide/en/elasticsearch/reference/current/install-elasticsearch.html)

[[https://www.elastic.co/guide/en/kibana/current/deb.html]{.underline}](https://www.elastic.co/guide/en/kibana/current/deb.html)

### Elastic

Mediante apt instalamos elastic previo a añadir su PGP y repositorio,
tal como indica la guía adjunta.

Configuramos el daemon para que arranque con el OS:

Comprobamos que esté levantado:

Configuramos de manera muy básica ELK:

(cambios realizados son las lineas '\<')

### Kibana

Instalamos mediante apt

Configuramos demonio, levantamos y hacemos BK de la config:

Copiamos el cert CA de Elastic a Kibana:

Reseteamos pass de kibana:

Realizamos configuración básica de Kibana:

Reseteamos el servicio de Kibana y comprobamos que levante
correctamente.

## Logstash

Ref:
https://www.elastic.co/guide/en/logstash/current/installing-logstash.html

1.  Actualizar lista de paquetes y actualizarlos

2.  Instalar 'openjdk-8-jre'\
    root@k8s-1:\~# apt-get install openjdk-8-jre

3.  Instalar la Public Signing Key, agregar el repo e instalar Logstash
    mediante APT

### Input

4.  Creamos un archivo de configuración exclusivo para el fortigate, nos
    vamos a basar sobre [[UDP input
    Plugin]{.underline}](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-udp.html)
    y a sacar el output por consola

5.  Configuramos el syslog en el Forti:

6.  Levantamos logstash con el archivo de config creado en el paso
    anterior para ***[examinar los logs que nos va a pasar el Forti\
    ]{.underline}***![](media/image4.png){width="6.5in"
    height="1.5972222222222223in"}\[..\]

### Filtros

[[https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html]{.underline}](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html)

7.  Copiamos un mensaje que contenga información del Forti para jugar
    con el Grok Debugger que podemos encontrar en Kibana.
    Explicación:\
    **%{SYSLOG5424PRI}\
    **Este es un patrón predefinido de Grok que corresponde a la
    prioridad de syslog en el formato RFC5424. Este formato incluye un
    número de prioridad encerrado entre \< y \>; en el ejemplo:
    \<189\>.\
    \
    **%{GREEDYDATA:message}**\
    Este es otro patrón predefinido de Grok. GREEDYDATA captura
    cualquier cantidad de caracteres hasta el final de la línea.\
    :message indica que todo lo que se capture con GREEDYDATA se
    almacenará en el campo llamado message.

8.  Borramos información que no interesa mediante mutate:

9.  Finalmente tras varias horas de prueba y error y de obtener
    diferentes fuentes de información el filtro queda así:\
    ![](media/image5.png){width="6.5in" height="6.75in"}\
    Usando KV borramos las contrabarras y agregamos un salto de línea.\
    Hecho una prueba con logdate y date, necesito más horas para
    desarrollar o debugear.\
    También encuentro útil convertir a Integer los bytes enviados y
    recibidos.

### Output

[[https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html]{.underline}](https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html)


Definimos el output filter de elasticsearch, indicando el hosts, creando
un index, especificando user y pass de elastic, e indicamos que CA debe
usar (la de Elastic).

### Configuración del servicio logstash

root@k8s-1:\~# systemctl start logstash.service

También comentamos la línea del filtro de output 'stdout' ya que no es
necesario ver la salida por consola más.

# Dashboards Kibana

Creamos un data view:

Una vez creado el data view, mediante Analytics -\> Discover es cuestión
de ir buscando la combinación de datos que resulte más interesante para
crear dashboards. Ejemplo:
